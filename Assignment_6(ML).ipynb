{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ioan-hedea/deepLearning4005/blob/main/Assignment_6(ML).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjvWWKxlXeO_"
      },
      "source": [
        "# **Machine Learning - Assignment 6**\n",
        "\n",
        "*These lab assignments are new in the Machine and Deep Learning course. We'd like to hear what you think!*\n",
        "\n",
        "*Please post any feedback you have on Brightspace. Thanks!*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgTz5tH7XePN"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fxce2fHzXePP"
      },
      "source": [
        "## Introduction - Evaluation in Machine Learning\n",
        "\n",
        "In this assignment, you will learn about common evaluation techniques in machine learning. This includes evaluating the error of the classifier, analyzing the learning curves, and performing cross-validation.\n",
        "\n",
        "**Prerequisites:**\n",
        "* Basic working knowledge of multivariate statistics and linear algebra\n",
        "* Basic knowledge of Python and Numpy. Recommended tutorial for Python and Numpy [here](https://cs231n.github.io/python-numpy-tutorial/).\n",
        "\n",
        "**Learning objectives:**\n",
        "* Can explain what sources of performance variability there are in machine learning\n",
        "* Knowing what a learning curve is\n",
        "* Can understand the difference between train and test error\n",
        "* Can explain what cross-validation is and how it is used\n",
        "\n",
        "**Exercises types:**\n",
        "* **Pen \\& Paper** - Some exercises will ask you to write down mathematical derivations, calculations, explanations, or simple plots and representations. You can perform these exercises on paper or using a LaTeX editor.\n",
        "* **Coding** - Some exercises will ask you to write Python code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOSpfO6mXePV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiGTLnkjXePa"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VOv7wDEXePc"
      },
      "source": [
        "## 1 - Sources of Variation (**Coding**)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQPkKh5NXePd"
      },
      "source": [
        "### **Exercise 1.1**\n",
        "\n",
        "In this exercise we investigate the difference in behavior of the error on the training and the test set. Generate a large test set and study the variations in the classification error based on repeatedly generated training sets. Use one of the artificial datasets from the first week's assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXlUFtrHXePe"
      },
      "outputs": [],
      "source": [
        "#TODO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1RAO2GOXePj"
      },
      "source": [
        "#### (a) What causes the variation in the error?\n",
        "\n",
        "Now do the same for different test sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOOJLLueXePk"
      },
      "outputs": [],
      "source": [
        "#TODO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fq0GCqfXePm"
      },
      "source": [
        "#### (b) Again explain what causes the variance observed in the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzZ7N6--XePn"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPUnKlIfXePn"
      },
      "source": [
        "## 2 - Learning Curves (**Coding**)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4ILgn-CXePo"
      },
      "source": [
        "### **Exercise 2.1**\n",
        "\n",
        "Using prtools lib, generate Highleyman classes (gendath) with a 1000 samples per class. Enlarge the feature dimensionality of this set by adding 60 dimensions of class independent randomness, i.e., plain noise. After that, use the function pr.cleval to generate learning curves for nmc, ldc, and qdc using 64, 128, 256, and 512 objects in the training set (make sure that you repeat often enough. . . ). Note that cleval automatically plots the learning curves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61W8vjWwXePo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b180802d-12a9-47b2-cfad-b4bdea50b526"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'prtools'...\n",
            "remote: Enumerating objects: 1119, done.\u001b[K\n",
            "remote: Counting objects: 100% (280/280), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 1119 (delta 264), reused 259 (delta 252), pack-reused 839 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1119/1119), 1.97 MiB | 11.13 MiB/s, done.\n",
            "Resolving deltas: 100% (747/747), done.\n",
            "Obtaining file:///content/prtools\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from prtools==1.2.1) (1.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from prtools==1.2.1) (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from prtools==1.2.1) (3.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from prtools==1.2.1) (2.32.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->prtools==1.2.1) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->prtools==1.2.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->prtools==1.2.1) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->prtools==1.2.1) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->prtools==1.2.1) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->prtools==1.2.1) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->prtools==1.2.1) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->prtools==1.2.1) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->prtools==1.2.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->prtools==1.2.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->prtools==1.2.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->prtools==1.2.1) (2024.8.30)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->prtools==1.2.1) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->prtools==1.2.1) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->prtools==1.2.1) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->prtools==1.2.1) (1.16.0)\n",
            "Installing collected packages: prtools\n",
            "  Running setup.py develop for prtools\n",
            "Successfully installed prtools-1.2.1\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/DMJTax/prtools.git\n",
        "!pip install -e prtools\n",
        "import prtools as pr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2d0XU3q5XePp"
      },
      "outputs": [],
      "source": [
        "#TODO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3LaZGZrXePq"
      },
      "source": [
        "#### (a) Can you explain the overall behavior of these curves?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a33QSxr-XePr"
      },
      "source": [
        "#### (b) Explain why the curves intersect. Which classifier performs best?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toUE7WIpXePr"
      },
      "source": [
        "#### (c) What do you expect the limiting behavior of learning curves is? That is, if we were able to   train on more and more data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSN5WNOtXePs"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptw7Av8aXePs"
      },
      "source": [
        "## 3 - Cross-Validation (**Coding**)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35bFDLE2XePt"
      },
      "source": [
        "### **Exercise 3.1**\n",
        "\n",
        "Generate a small data set, say, with 10 objects per class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fr-wd0_5XePt"
      },
      "outputs": [],
      "source": [
        "#TODO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRbooyJeXePu"
      },
      "source": [
        "#### (a) Using n-fold cross-validation,make plots for the error rates for kNN and 1NN over different values of n. Also calculate the standard deviation of the error estimate, e.g., by performing the cross-validation 10 times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wu7_Iz_3XePv"
      },
      "outputs": [],
      "source": [
        "#TODO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99TPo8ayXePw"
      },
      "source": [
        "#### (b) What do you notice about the estimated error rates? What is the general trend (maybe you should redo the data generation and the cross-validation a couple of times)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSFOpzd6XePw"
      },
      "source": [
        "#### (c) What happens to the variance of the estimates for varying n? Again, we are interested in the general trend."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzYs6xZAXePx"
      },
      "source": [
        "#### (d) How would the observations change if one would repeat the experiments with much larger dataset? Would they change?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}