{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dMiT4F2QfhSe"
   },
   "source": [
    "# **Machine Learning - Assignment 1**\n",
    "\n",
    "*These lab assignments are new in the Machine and Deep Learning course. We'd like to hear what you think!*\n",
    "\n",
    "*Please post any feedback you have on Brightspace. Thanks!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWm0gUf3fhSh"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmLKU2QHfhSh"
   },
   "source": [
    "## Introduction - Basics of Machine Learning\n",
    "\n",
    "In this assignment, you will go through some of the basic concepts of machine learning. You will learn about the Bayes' rule, decision boundaries, and how to classify simple datasets. You will also get familiar with the PRTools library, which is a toolbox for pattern recognition in Python.\n",
    "\n",
    "**Prerequisites:**\n",
    "* Basic working knowledge of multivariate statistics and linear algebra\n",
    "* Basic knowledge of Python and Numpy. Recommended tutorial for Python and Numpy [here](https://cs231n.github.io/python-numpy-tutorial/).\n",
    "\n",
    "**Learning objectives:**\n",
    "* Should be able to mathematically derive decision boundaries given simple probability density functions\n",
    "* Should be able to perform simple computations with Bayes’ rule\n",
    "* Should be familiar with working under Python\n",
    "* Understand some PRTools commands\n",
    "* Know what an object and a dataset are\n",
    "* Should be able to construct, visualize and classify some simple datasets\n",
    "\n",
    "**Exercises types:**\n",
    "* **Pen \\& Paper** - Some exercises will ask you to write down mathematical derivations, calculations, explanations, or simple plots and representations. You can perform these exercises on paper or using a LaTeX editor.\n",
    "* **Coding** - Some exercises will ask you to write Python code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TYmD4x_fhSi"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YSJx5UyPfhSi"
   },
   "source": [
    "## 1 - Decision Theory (**Pen \\& Paper**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvlappAsfhSj"
   },
   "source": [
    "### **Exercise 1.1**\n",
    "\n",
    "#### (a) Gaussian Distributions with Equal Priors\n",
    "\n",
    "Assume that we managed to represent objects from a two-class classification problem by a single feature. We know that the objects from class $ \\omega_1 $ have a Gaussian distribution with $ \\mu_1 = 0 $ and $ \\sigma_1^2 = \\frac{1}{2} $, and the objects from class $ \\omega_2 $ have a Gaussian distribution with $ \\mu_2 = 1 $ and $ \\sigma_2^2 = \\frac{1}{2} $.\n",
    "\n",
    "Derive the position of the decision boundary when both class priors are equal.\n",
    "\n",
    "#### (b) Uniform Distributions between Different Ranges\n",
    "\n",
    "Again, assume we have a two-class classification problem in a 1D feature space, but now assume that objects from class $ \\omega_1 $ have a uniform distribution between 0 and 1, and objects from class $ \\omega_2 $ have a uniform distribution between 2 and 3.\n",
    "\n",
    "Where is the decision boundary now?\n",
    "\n",
    "#### (c) Decision Boundary with Overlapping Uniform Distributions\n",
    "\n",
    "Where is the decision boundary when the objects from class $ \\omega_2 $ have a uniform distribution between 0.5 and 1.5? (The distribution of $ \\omega_1 $ did not change, classes have equal prior.)\n",
    "\n",
    "#### (d) Decision Boundary with Extended Uniform Distributions\n",
    "\n",
    "And where is the decision boundary when the objects from class $ \\omega_2 $ have a uniform distribution between 0.5 and 2.5? (The distribution of $ \\omega_1 $ did not change, classes have equal prior.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtYbPgCJfhSj"
   },
   "source": [
    "### **Exercise 1.2**\n",
    "\n",
    "#### (a) Decision Boundary with Loss Matrix\n",
    "\n",
    "Assume we represent the objects in a two-class classification problem by a single feature. We know that the objects from class $ \\omega_1 $ have a Gaussian distribution with $ \\mu_1 = 0 $ and $ \\sigma_1^2 = \\frac{1}{2} $, and the objects from class $ \\omega_2 $ have a Gaussian distribution with $ \\mu_2 = 1 $ and $ \\sigma_2^2 = \\frac{1}{2} $. Derive the position of the decision boundary when both class priors are equal, but we have a loss matrix of:\n",
    "\n",
    "$$\n",
    "L = \\begin{pmatrix}\n",
    "0 & 0.5 \\\\\n",
    "1.0 & 0\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "#### (b) Decision Boundary with Loss Matrix for Overlapping Uniform Distributions\n",
    "\n",
    "Assume again we have a two-class classification problem in a 1D feature space, but now assume that objects from class $ \\omega_1 $ have a uniform distribution between 0 and 1, and objects from class $ \\omega_2 $ have a uniform distribution between 0.5 and 2.5. Given the loss matrix (1.1), where is the decision boundary now?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJKwDBqmfhSk"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4l-frpPfhSj"
   },
   "source": [
    "### **Exercise 1.3**\n",
    "\n",
    "In the figure below, two triangular-shaped class conditional probability density functions are given. The first one $ p(x|\\omega_1) $ is indicated by a dashed blue line and the second $ p(x|\\omega_2) $ with a solid black line. The class priors are assumed equal here.\n",
    "\n",
    "![Local Image](https://drive.usercontent.google.com/download?id=1boxOLn2-4amJ-E8smrAiDMNEfE8tF9LE&export=view&authuser=0)\n",
    "\n",
    "#### (a) Class Posterior Probabilities Using Bayes' Rule\n",
    "\n",
    "Use the Bayes' rule to derive the class posterior probabilities of the following objects: $ x = 3 $, $ x = -0.5 $, $ x = +0.5 $. To which class are the objects therefore assigned?\n",
    "\n",
    "#### (b) Decision Boundary of the Bayes Classifier\n",
    "\n",
    "Which object is on the decision boundary of the Bayes classifier?\n",
    "\n",
    "### **Exercise 1.4**\n",
    "\n",
    "Now assume that class $ \\omega_1 $ in the figure is twice as small as class $ \\omega_2 $. That means $ p(\\omega_1) = \\frac{1}{3} $ and $ p(\\omega_2) = \\frac{2}{3} $.\n",
    "\n",
    "#### (a) Posterior Probabilities with Updated Priors\n",
    "\n",
    "Compute the posterior probabilities for $ x = 3 $, $ x = -0.5 $, $ x = 0.5 $.\n",
    "\n",
    "#### (b) Decision Boundary with Updated Priors\n",
    "\n",
    "Where is the decision boundary of the Bayes classifier now?\n",
    "\n",
    "### **Exercise 1.5**\n",
    "\n",
    "Compute the Bayes error for the class distributions given in the figure above, where the classes have equal prior.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZ68KDXAfhSk"
   },
   "source": [
    "Before moving on to the coding exercises for this week, let's first revise some basic concepts of the PRTools library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yriXmiRAfhSk"
   },
   "source": [
    "### **Tutorial** - PRTools Basics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jC4yhBArfhSk",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "658ae085-a219-4a83-d091-8a8aed20a23c",
    "ExecuteTime": {
     "end_time": "2025-09-02T15:29:23.540615Z",
     "start_time": "2025-09-02T15:29:20.951723Z"
    }
   },
   "source": [
    "!pip install git+https://github.com/DMJTax/prtools.git\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/DMJTax/prtools.git\r\n",
      "  Cloning https://github.com/DMJTax/prtools.git to /private/var/folders/41/9hldc1z978zfcdj5cftx6n5c0000gn/T/pip-req-build-f2jqjxds\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/DMJTax/prtools.git /private/var/folders/41/9hldc1z978zfcdj5cftx6n5c0000gn/T/pip-req-build-f2jqjxds\r\n",
      "  Resolved https://github.com/DMJTax/prtools.git to commit 38dee74c266c54a1c75318390edc21a87668264f\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: scikit-learn in /opt/homebrew/lib/python3.10/site-packages (from prtools==1.2.1) (1.2.1)\r\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.10/site-packages (from prtools==1.2.1) (1.24.2)\r\n",
      "Requirement already satisfied: matplotlib in /opt/homebrew/lib/python3.10/site-packages (from prtools==1.2.1) (3.7.0)\r\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.10/site-packages (from prtools==1.2.1) (2.31.0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/lib/python3.10/site-packages (from matplotlib->prtools==1.2.1) (1.0.7)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/lib/python3.10/site-packages (from matplotlib->prtools==1.2.1) (0.11.0)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/lib/python3.10/site-packages (from matplotlib->prtools==1.2.1) (4.38.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/homebrew/lib/python3.10/site-packages (from matplotlib->prtools==1.2.1) (1.4.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/lib/python3.10/site-packages (from matplotlib->prtools==1.2.1) (23.0)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/homebrew/lib/python3.10/site-packages (from matplotlib->prtools==1.2.1) (9.4.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/lib/python3.10/site-packages (from matplotlib->prtools==1.2.1) (3.0.9)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/homebrew/lib/python3.10/site-packages (from matplotlib->prtools==1.2.1) (2.8.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->prtools==1.2.1) (1.16.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.10/site-packages (from requests->prtools==1.2.1) (3.0.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.10/site-packages (from requests->prtools==1.2.1) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.10/site-packages (from requests->prtools==1.2.1) (1.26.14)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.10/site-packages (from requests->prtools==1.2.1) (2022.12.7)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/homebrew/lib/python3.10/site-packages (from scikit-learn->prtools==1.2.1) (1.10.1)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/homebrew/lib/python3.10/site-packages (from scikit-learn->prtools==1.2.1) (1.2.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/lib/python3.10/site-packages (from scikit-learn->prtools==1.2.1) (3.1.0)\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vDc1jr-SfhSm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import prtools as pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xzL5lqjUfhSm"
   },
   "outputs": [],
   "source": [
    "# Use the following code to create a dummy dataset containing 10 objects with 2 features each.\n",
    "\n",
    "x = np.array(\n",
    "    [[1, 5], [4, 5], [7, 8],\n",
    "    [10, 11], [11, 14], [16, 17],\n",
    "    [19, 20], [22, 2], [25, 23],\n",
    "    [28, 20]]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "adFuF1LbfhSm",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7e4ffe05-9d4f-41eb-eb5f-6413579a22dd"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean: [14.3 12.5]\n",
      "Std: [8.67236992 7.00357052]\n",
      "\n",
      "Mean: [ 3.   4.5  7.5 10.5 12.5 16.5 19.5 12.  24.  24. ]\n",
      "Std: [ 2.   0.5  0.5  0.5  1.5  0.5  0.5 10.   1.   4. ]\n",
      "\n",
      "Mean: 13.4\n",
      "Std: 7.933473388119481\n"
     ]
    }
   ],
   "source": [
    "#To compute the means and std of the features, you can use the available numpy functions.\n",
    "\n",
    "# The 'axis' parameter specifies the dimension along which the mean/std should be computed.\n",
    "\n",
    "# If axis=0, the mean/std is computed for each feature across all objects.\n",
    "\n",
    "mean = np.mean(x, axis=0)\n",
    "std = np.std(x, axis=0)\n",
    "\n",
    "print('Mean:', mean)\n",
    "print('Std:', std)\n",
    "print()\n",
    "\n",
    "# If axis=1, the mean/std is computed for each object across all features.\n",
    "\n",
    "mean = np.mean(x, axis=1)\n",
    "std = np.std(x, axis=1)\n",
    "\n",
    "print('Mean:', mean)\n",
    "print('Std:', std)\n",
    "print()\n",
    "\n",
    "# If the parameter is not specified, the mean/std is computed for all elements in the array.\n",
    "\n",
    "mean = np.mean(x)\n",
    "std = np.std(x)\n",
    "\n",
    "print('Mean:', mean)\n",
    "print('Std:', std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iWIXrPECfhSn"
   },
   "outputs": [],
   "source": [
    "#When a dataset contains just two features per object, it can be visualized in a scatterplot\n",
    "\n",
    "plt.scatter(x[:, 0], x[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZG2XUPLxfhSn",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "eb16e623-e30c-4fb1-f034-c9fb3e4376d3"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10 by 2 prdataset with 2 classes: [4 6]\n"
     ]
    }
   ],
   "source": [
    "#You can create labels for the previously generated data by defining another numpy array\n",
    "\n",
    "lab = np.array([1,1,1,1,2,2,2,2,2,2]).reshape(10,1)\n",
    "#lab = np.array([[1,1,1,1,2,2,2,2,2,2]]).T # alternative\n",
    "\n",
    "#Then, you can create a PRTools dataset using the prdataset function\n",
    "\n",
    "a = pr.prdataset(x,lab)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S4MZLwOTfhSn"
   },
   "outputs": [],
   "source": [
    "#If visualized, it will show the labels in different colors\n",
    "\n",
    "pr.scatterd(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mXBPlCSxfhSn"
   },
   "outputs": [],
   "source": [
    "#There is a set of predefined datasets in PRTools that can be loaded using\n",
    "#functions as gendatb, gendatc, gendats ...\n",
    "\n",
    "#For instance, to load a Banana shaped dataset, you can use the following line\n",
    "\n",
    "b = pr.gendatb()\n",
    "print(b)\n",
    "\n",
    "#... and then visualize it\n",
    "pr.scatterd(b)\n",
    "\n",
    "#Other datasets that can be loaded in the same way:\n",
    "\n",
    "# - gendatc: generation of circular classes\n",
    "# - gendats: generation of two Gaussian classes\n",
    "# - gendatd: generation of two difficult classes\n",
    "# - gendath: generation of the hygleyman dataset\n",
    "# ... and more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ukakzJxGfhSo"
   },
   "outputs": [],
   "source": [
    "#Assume we want to train a nearest mean classifier on the defined Banana dataset. In PRTools you do:\n",
    "\n",
    "w = pr.nmc()\n",
    "w.train(b)\n",
    "print(w) # this will show the classifier parameters\n",
    "\n",
    "c = w.eval(b)\n",
    "print(c)\n",
    "\n",
    "e = pr.testc(c)\n",
    "print(e) # this will show the classification error\n",
    "\n",
    "#to visualize the decision boundary of the classifier, you can use the following code\n",
    "\n",
    "pr.scatterd(b)\n",
    "pr.plotc(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0QHkfU3efhSo"
   },
   "outputs": [],
   "source": [
    "#Some classifiers require additional hyperparameters to be specified.\n",
    "#For instance, in the support vector classifier, you can specify the kernel, a kernel parameter, and a regularisation parameter.\n",
    "#Or in the k-nearest neighbor classifier you can specify the number of neighbors k.\n",
    "#You can supply that as additional input during training. For example, for svc:\n",
    "\n",
    "w = pr.svc(('rbf',4.5,1 ))\n",
    "w.train(b)\n",
    "print(w)\n",
    "\n",
    "c = w.eval(b)\n",
    "print(c)\n",
    "\n",
    "e = pr.testc(c)\n",
    "print(e)\n",
    "\n",
    "pr.scatterd(b)\n",
    "pr.plotc(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VZt3aiqyfhSo"
   },
   "outputs": [],
   "source": [
    "#... or for knnc:\n",
    "\n",
    "w = pr.knnc(3)\n",
    "w.train(b)\n",
    "print(w)\n",
    "\n",
    "c = w.eval(b)\n",
    "print(c)\n",
    "\n",
    "e = pr.testc(c)\n",
    "print(e)\n",
    "\n",
    "pr.scatterd(b)\n",
    "pr.plotc(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-C4e4tycfhSo"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RH6DN9r3fhSo"
   },
   "source": [
    "## 2 - Creating Maps and Decision Boundaries (**Coding**)\n",
    "\n",
    "So far, we have visualized the decision boundary and computed the classification error on a the training set. However, the classification error on the training set is not a good indicator of the performance of the classifier. Here is a complete example of training a svc classifier and visualizing the decision boundary and classification error on a test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p_ukd2sOfhSp"
   },
   "outputs": [],
   "source": [
    "#Create hard training dataset using gendatd\n",
    "\n",
    "d = pr.gendatd()\n",
    "print(d)\n",
    "\n",
    "#Train a ldc classifier on the dataset\n",
    "\n",
    "w = pr.svc()\n",
    "w.train(d)\n",
    "\n",
    "#Evaluate the classifier on the training set\n",
    "\n",
    "c = w.eval(d)\n",
    "print(c)\n",
    "\n",
    "e = pr.testc(c)\n",
    "print(e)\n",
    "\n",
    "#Visualize the training set\n",
    "\n",
    "pr.scatterd(d)\n",
    "pr.plotc(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ifF1k-SHfhSp"
   },
   "outputs": [],
   "source": [
    "#Create a test dataset using gendatd\n",
    "\n",
    "t = pr.gendatd()\n",
    "print(t)\n",
    "\n",
    "#Evaluate the classifier on the test set\n",
    "\n",
    "c = w.eval(t)\n",
    "print(c)\n",
    "\n",
    "e = pr.testc(c)\n",
    "print(e)\n",
    "\n",
    "#Visualize the test set\n",
    "\n",
    "pr.scatterd(t)\n",
    "pr.plotc(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxIviYKEfhSp"
   },
   "source": [
    "### **Exercise 2.1**\n",
    "\n",
    "Train the following classifiers on some 2D datasets and visualize the decision boundaries. Also visualize the error on the test set.\n",
    "\n",
    "**Classifiers:**\n",
    "* ldc -> Linear discriminant analysis\n",
    "* qdc -> Quadratic discriminant analysis\n",
    "* nmc -> Nearest mean classifier\n",
    "* fisherc -> Fishers linear discriminant\n",
    "* knnc -> k-nearest neighbor classifier\n",
    "* parzenc -> Parzen classifier\n",
    "* naivebc -> Naive-Bayes classifier\n",
    "* mogc -> Mixture-of-Gaussians classifier\n",
    "* stumpc -> Decision stump classifier\n",
    "* dectreec -> Decision tree classifier\n",
    "* adaboostc -> AdaBoost\n",
    "* svc -> Support vector classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m3wHs2bcfhSp"
   },
   "outputs": [],
   "source": [
    "#Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYGg-bg5fhSp"
   },
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wn09cvyGfhSp"
   },
   "source": [
    "## 3 - Classification with Normal Densities (**Coding**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqhoaMLyfhSq"
   },
   "source": [
    "### **Exercise 3.1**\n",
    "\n",
    "Consider two 2D normal distributions with different means but equal covariance matrices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLo9loxufhSq"
   },
   "source": [
    "#### (a) Shape of the Optimal Decision Boundary\n",
    "\n",
    "What shape does the optimal decision boundary have?\n",
    "\n",
    "Generate 10 data points per class from a data set that fulfills the above assumptions. Create a data set with these 20 points and these two classes. We can estimate linear discriminants by means of `ldc(a)` and quadratic discriminant boundaries by means of `qdc(a)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wxZs9ESafhSq"
   },
   "outputs": [],
   "source": [
    "#Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5z9d83XfhSq"
   },
   "source": [
    "#### (b) Shape of the Quadratic Discriminant Boundary\n",
    "\n",
    "Given the shape of the optimal decision boundary, how does the boundary look for the trained normal density-based quadratic discriminant?\n",
    "\n",
    "The PRTools command `plotc` plots the decision boundary. If you plotted decision boundaries of multiple classifiers in one plot command `pr.plotc([w1, w2])`, you will get a legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XBC06attfhSq"
   },
   "outputs": [],
   "source": [
    "#Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sGmHZbsVfhSr"
   },
   "source": [
    "#### (c) Scatter Plot and Decision Boundaries\n",
    "\n",
    "Scatter the data set and plot the decision boundary estimated by means of `ldc(a)` and `qdc(a)`. Can you explain their difference? You might want to revisit the previous question (b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e6kCqUUofhSr"
   },
   "outputs": [],
   "source": [
    "#Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwEoNIzdfhSr"
   },
   "source": [
    "#### (d) Increasing Number of Data Points\n",
    "\n",
    "What happens when the number of points per class increases? What happens in the limit of an infinite number of points?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCvKl0MXfhSs"
   },
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_D20VflfhSs"
   },
   "source": [
    "## 4 - Density estimation using Parzen densities **(Coding)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahmNJxJafhSs"
   },
   "source": [
    "Next to classifiers, PRTools also has the possibility to estimate densities. In this section we are going to estimate the density using a Parzen density estimator, called parzenm in PRTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gUjTCsUPfhSt"
   },
   "outputs": [],
   "source": [
    "#We start with creating a simple dataset with:\n",
    "a = pr.gendats([20,20],1,8)\n",
    "\n",
    "#We define the width parameter h for the Parzen kernel:\n",
    "h = 0.5\n",
    "\n",
    "#The function parzenm estimates a density for a given dataset. In most cases a PRTools prdataset is labeled, and these labels are used in the function parzenm to estimate a density for each class. To define a Parzen density estimator with a certain width parameter h on the entire dataset, ignoring labels, type:\n",
    "a = pr.prdataset(+a)\n",
    "w = pr.parzenm(a,h)\n",
    "\n",
    "#This mapping can now be plotted along with the data:\n",
    "pr.scatterd(a)\n",
    "pr.plotm(w)\n",
    "\n",
    "#If your graphs look a little “bumpy”, you can increase the grid size PRTools uses for plotting:\n",
    "\n",
    "pr.plotm(w,gridsize=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxbEObydfhSt"
   },
   "source": [
    "When you want to evaluate the fit of a density model to some data, you have to define an error. One possibility is to use the log-likelihood, defined as:\n",
    "\n",
    "$$\n",
    "LL(X) = \\log \\prod_i \\hat{p}(x_i) = \\sum_i \\log(\\hat{p}(x_i)) \\tag{1.2}\n",
    "$$\n",
    "\n",
    "The better the data $x$ fits in the probability density model $\\hat{p}$, the higher the values of $\\hat{p}(x)$ will be. This will result in a high value of $\\sum_i \\log(\\hat{p}(x_i))$. When we have different probability density estimates $\\hat{p}$, we use the one which has the highest value of $LL$.\n",
    "\n",
    "Note that when we fill in different values for the width parameters $h$ in the Parzen density estimation, we have different estimates $\\hat{p}$. Using the log-likelihood as a criterion, we can optimize the value of this free parameter $h$ to maximize $LL$.\n",
    "\n",
    "To get an honest estimate of the log-likelihood, we have to evaluate the log-likelihood (1.2) on a test set. That means that we have to make (or measure) new data from the same distribution as where the training data came from. When we evaluate the log-likelihood on the data on which the probability density was fitted, we would get a too optimistic estimation of the error. We might conclude that we have fitted the data very well, while actually a new dataset from the same distribution does not fit in the density at all! Therefore, if you want to evaluate the performance of an estimated $\\hat{p}$, use an independent test set!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNkfXRdTfhSu"
   },
   "source": [
    "### **Exercise 4.1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4DgiFIxfhSu"
   },
   "source": [
    "Use the data from the same distribution as in the previous exercise to train a Parzen density estimator for different values of h. Compute the log-likelihood of this training set given the estimated densities (for different $h$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2U5SxoM-fhSu"
   },
   "outputs": [],
   "source": [
    "a = pr.gendats([20,20],1,8)                 # Generate data\n",
    "a = pr.prdataset(+a)\n",
    "hs = [0.01,0.05,0.1,0.25,0.5,1,1.5,2,3,4,5] # Array of h’s to try\n",
    "LL = np.zeros(len(hs))\n",
    "for i in range(len(hs)):\n",
    "                                         # For each h...\n",
    "    w = pr.parzenm(a,hs[i])                  #   estimate Parzen density\n",
    "    LL[i] = np.sum(np.log(+(a*w)));          #   calculate log-likelihood\n",
    "plt.plot(hs,LL)\n",
    "plt.xlabel('h')\n",
    "plt.ylabel('log-likelihood')\n",
    "plt.title('Parzen density estimation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fGfJi07jfhSu"
   },
   "source": [
    "What is the optimal value for h, i.e. the maximal likelihood? Is this also the best density estimate for the dataset?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEVOdkBJfhSu"
   },
   "source": [
    "### **Exercise 4.2**\n",
    "\n",
    "Use the same data as in the previous exercise, but now split the data into a training and test set of equal size. Estimate a Parzen density on the training set and compute the Parzen density for the test set. Compute the log-likelihood on both the training and test sets for $ h = [0.1, 0.25, 0.5, 1, 1.5, 2, 3, 4, 5] $. Plot these log-likelihood vs. $ h $ curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZzP2aCFsfhSu"
   },
   "outputs": [],
   "source": [
    "# Split into trn and tst, both 50%\n",
    "[trn, tst] = pr.gendat(a, 0.5)\n",
    "\n",
    "# Array of h’s to try\n",
    "hs = [0.01, 0.05, 0.1, 0.25, 0.5, 1, 1.5, 2, 3, 4, 5]\n",
    "\n",
    "# Initialize log-likelihood arrays\n",
    "Ltrn = np.zeros(len(hs))\n",
    "Ltst = np.zeros(len(hs))\n",
    "\n",
    "# For each h...\n",
    "for i in range(len(hs)):\n",
    "    # Estimate Parzen density\n",
    "    w = pr.parzenm(trn, hs[i])\n",
    "\n",
    "    # Calculate trn log-likelihood\n",
    "    Ltrn[i] = np.sum(np.log(+(trn * w)))\n",
    "\n",
    "    # Calculate tst log-likelihood\n",
    "    Ltst[i] = np.sum(np.log(+(tst * w)))\n",
    "\n",
    "# Plot log-likelihood as function of h\n",
    "plt.plot(hs, Ltrn, 'b-', label='Training Log-Likelihood')  # Plot trn log-likelihood as function of h\n",
    "plt.plot(hs, Ltst, 'r-', label='Test Log-Likelihood')      # Plot tst log-likelihood as function of h\n",
    "plt.xlabel('h')\n",
    "plt.ylabel('Log-Likelihood')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5P_symSfhSv"
   },
   "source": [
    "What is a good choice for $ h $?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rymE6Aa_fhSv"
   },
   "source": [
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
